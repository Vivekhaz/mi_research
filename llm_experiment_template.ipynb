{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: $EXPERIMENT NAME\n",
    "## Last Updated $DATE -  $AUTHOR.\n",
    "\n",
    "```\n",
    "Summary of High Level Research Question\n",
    "```\n",
    "\n",
    "Try to scope your experiments such you can answer your research question in 1-3 hours.\n",
    "This is an ideal time block to enter flow / deep work, but short enough that you will still feel \n",
    "motivated by a relatively tight feedback loop.\n",
    "\n",
    "If a problem seems like it needs more time that that, \n",
    "\n",
    "### High Level Experiment Design\n",
    "\n",
    "## Goals:\n",
    "```\n",
    "List of specific goals that this experiment seeks to achieve.\n",
    "\n",
    "This should fall under a few categories:\n",
    "- Development of Intuition about a _specific_ topic\n",
    "- Novel Research or Insight that could lead to a publishable result\n",
    "- Meaningfully explore a topic which could lead to an improvement in product\n",
    "\n",
    "Guiding principles should understanding, insight, and value creation.\n",
    "```\n",
    "\n",
    "## Tasks & Experiment Design\n",
    "\n",
    "```\n",
    "A list of specific tasks that are going to be tested \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "```\n",
    "Document high level research findings and how\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \n",
    "# %pip install circuitsvis\n",
    "# %pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Set of Imports for MI Research\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch configuration for inference based experiments\n",
    "# NOTE: Mark as False if you want to do any kind of training \n",
    "#       as part of your experimentation\n",
    "\n",
    "INFERENCE_ONLY_EXPERIMENT = True\n",
    "if INFERENCE_ONLY_EXPERIMENT:\n",
    "    torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-f16bdd29-9bd6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f16bdd29-9bd6\",\n",
       "      Hello,\n",
       "      {\"name\": \"Vivek\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fb091e7bc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Circuit Visualizations\n",
    "# TODO: Explore building out our own packages / tooling\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Vivek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hf_model, hooking transformer into TransformerLens!\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50432,\n",
      " 'd_vocab_out': 50432,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "Loaded pretrained model into HookedTransformer!\n",
      "Model loss: tensor(4.5184, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Load & Run a Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "\n",
    "print(\"Loaded hf_model, hooking transformer into TransformerLens!\")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"EleutherAI/pythia-6.9b-deduped\",\n",
    "#     center_unembed=False,\n",
    "#     center_writing_weights=False,\n",
    "#     fold_ln=False,\n",
    "#     refactor_factored_attn_matrices=True,\n",
    "#     hf_model=hf_model\n",
    "# )\n",
    "\n",
    "### Janky Shit\n",
    "### TODO: Figure out how this library actually works and make this a cleaner integration.\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "# Get the model name used in HuggingFace, rather than the alias.\n",
    "official_model_name = loading.get_official_model_name(\"EleutherAI/pythia-6.9b-deduped\")\n",
    "\n",
    "\n",
    "# Load the config into an HookedTransformerConfig object. If loading from a\n",
    "# checkpoint, the config object will contain the information about the\n",
    "# checkpoint\n",
    "cfg = loading.get_pretrained_model_config(\n",
    "    official_model_name,\n",
    "    checkpoint_index=None,\n",
    "    checkpoint_value=None,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=1,\n",
    ")\n",
    "print(cfg)\n",
    "cfg.d_vocab = 50280\n",
    "cfg.d_vocab_out = 50280\n",
    "print(cfg)\n",
    "\n",
    "\n",
    "# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the HookedTransformer parameter names.\n",
    "state_dict = loading.get_pretrained_state_dict(\n",
    "    official_model_name, cfg, hf_model\n",
    ")\n",
    "\n",
    "# Create the HookedTransformer object\n",
    "model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "\n",
    "model.load_and_process_state_dict(\n",
    "    state_dict,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    move_state_dict_to_device=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded pretrained model into HookedTransformer!\")\n",
    "\n",
    "model_description_text = \"\"\"For this demo notebook we'll look at Dolly v2. It is based on pythia 6.9b, but we use the weights for dolly v2. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "# return_type of model can be loss, logits, both, or none!\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'the', ' founder', ' of', ' Facebook', ' is', ' Mark']\n",
      "Tokenized answer: [' Z', 'ucker', 'berg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.91</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.94</span><span style=\"font-weight: bold\">% Token: | Z|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m22.91\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.94\u001b[0m\u001b[1m% Token: | Z|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 22.91 Prob: 99.94% Token: | Z|\n",
      "Top 1th token. Logit: 15.02 Prob:  0.04% Token: | z|\n",
      "Top 2th token. Logit: 12.46 Prob:  0.00% Token: | E|\n",
      "Top 3th token. Logit: 12.16 Prob:  0.00% Token: | Elliot|\n",
      "Top 4th token. Logit: 11.97 Prob:  0.00% Token: |\n",
      "|\n",
      "Top 5th token. Logit: 11.81 Prob:  0.00% Token: | Cuban|\n",
      "Top 6th token. Logit: 11.52 Prob:  0.00% Token: |Z|\n",
      "Top 7th token. Logit: 11.40 Prob:  0.00% Token: |  |\n",
      "Top 8th token. Logit: 10.68 Prob:  0.00% Token: |us|\n",
      "Top 9th token. Logit: 10.56 Prob:  0.00% Token: |.|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.77</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.60</span><span style=\"font-weight: bold\">% Token: |ucker|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m23.77\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.60\u001b[0m\u001b[1m% Token: |ucker|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 23.77 Prob: 99.60% Token: |ucker|\n",
      "Top 1th token. Logit: 17.93 Prob:  0.29% Token: |uk|\n",
      "Top 2th token. Logit: 16.55 Prob:  0.07% Token: |uck|\n",
      "Top 3th token. Logit: 15.53 Prob:  0.03% Token: |uc|\n",
      "Top 4th token. Logit: 13.66 Prob:  0.00% Token: |UCK|\n",
      "Top 5th token. Logit: 12.91 Prob:  0.00% Token: |.|\n",
      "Top 6th token. Logit: 11.87 Prob:  0.00% Token: |ub|\n",
      "Top 7th token. Logit: 11.50 Prob:  0.00% Token: |ucks|\n",
      "Top 8th token. Logit: 11.00 Prob:  0.00% Token: |ander|\n",
      "Top 9th token. Logit: 10.94 Prob:  0.00% Token: |im|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.24</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.27</span><span style=\"font-weight: bold\">% Token: |berg|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m25.24\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m98.27\u001b[0m\u001b[1m% Token: |berg|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 25.24 Prob: 98.27% Token: |berg|\n",
      "Top 1th token. Logit: 21.15 Prob:  1.65% Token: |burg|\n",
      "Top 2th token. Logit: 17.57 Prob:  0.05% Token: |ber|\n",
      "Top 3th token. Logit: 16.38 Prob:  0.01% Token: |borg|\n",
      "Top 4th token. Logit: 15.81 Prob:  0.01% Token: |beg|\n",
      "Top 5th token. Logit: 13.86 Prob:  0.00% Token: |bert|\n",
      "Top 6th token. Logit: 13.72 Prob:  0.00% Token: |bur|\n",
      "Top 7th token. Logit: 13.15 Prob:  0.00% Token: |­|\n",
      "Top 8th token. Logit: 12.94 Prob:  0.00% Token: |b|\n",
      "Top 9th token. Logit: 12.89 Prob:  0.00% Token: |berger|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Z'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ucker'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'berg'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Z'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'ucker'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'berg'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "example_prompt = \"the founder of Facebook is Mark\"\n",
    "example_answer = \"Zuckerberg\"\n",
    "\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = model.to_tokens(example_prompt, prepend_bos=True)\n",
    "# Move the tokens to the GPU\n",
    "tokens = tokens.cuda()\n",
    "# Run the model and cache all activations\n",
    "original_logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "\n",
    "# Different Strategies for Decoding Text\n",
    "\n",
    "## Greedy\n",
    "\n",
    "## Sampling\n",
    "\n",
    "## Top K Sampling\n",
    "\n",
    "## Nucleus Sampling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m answer_logit_diff\u001b[39m.\u001b[39mmean()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPer prompt logit difference:\u001b[39m\u001b[39m\"\u001b[39m, logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m original_average_logit_diff \u001b[39m=\u001b[39m logits_to_ave_logit_diff(original_logits, answer_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Volumes/git/llm-research/notebooks/llm_experiment_template.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage logit difference:\u001b[39m\u001b[39m\"\u001b[39m, logits_to_ave_logit_diff(original_logits, answer_tokens)\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True))\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
    "print(\"Average logit difference:\", logits_to_ave_logit_diff(original_logits, answer_tokens).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate an Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate an Neuron\n",
    "LAYER_TO_ABLATE = 0\n",
    "MLP_NEURON_TO_ABLATE = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e562f6d96a8844f2887829ab9fd8d496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'On halloween, all the children go Trick or Treating. My son is not a fan of'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"On halloween, all the children go Trick or\", \n",
    "               temperature=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
