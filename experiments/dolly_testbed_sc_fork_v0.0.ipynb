{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Working with Dolly\n",
    "## Last Updated $DATE -  $AUTHOR.\n",
    "\n",
    "```\n",
    "Summary of High Level Research Question\n",
    "```\n",
    "\n",
    "Try to scope your experiments such you can answer your research question in 1-3 hours.\n",
    "This is an ideal time block to enter flow / deep work, but short enough that you will still feel \n",
    "motivated by a relatively tight feedback loop.\n",
    "\n",
    "If a problem seems like it needs more time that that, \n",
    "\n",
    "### High Level Experiment Design\n",
    "\n",
    "## Goals:\n",
    "```\n",
    "List of specific goals that this experiment seeks to achieve.\n",
    "\n",
    "This should fall under a few categories:\n",
    "- Development of Intuition about a _specific_ topic\n",
    "- Novel Research or Insight that could lead to a publishable result\n",
    "- Meaningfully explore a topic which could lead to an improvement in product\n",
    "\n",
    "Guiding principles should understanding, insight, and value creation.\n",
    "```\n",
    "\n",
    "## Tasks & Experiment Design\n",
    "\n",
    "```\n",
    "A list of specific tasks that are going to be tested \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "```\n",
    "Document high level research findings and how\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install things into ENV\n",
    "# TODO: Setup up a container and push to docker that contains all these\n",
    "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "%pip install circuitsvis\n",
    "%pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Set of Imports for MI Research\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyTorch configuration for inference based experiments\n",
    "# NOTE: Mark as False if you want to do any kind of training \n",
    "#       as part of your experimentation\n",
    "\n",
    "INFERENCE_ONLY_EXPERIMENT = True\n",
    "if INFERENCE_ONLY_EXPERIMENT:\n",
    "    torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circuit Visualizations\n",
    "# TODO: Explore building out our own packages / tooling\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Vivek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hf_model, hooking transformer into TransformerLens!\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50432,\n",
      " 'd_vocab_out': 50432,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "Loaded pretrained model into HookedTransformer!\n",
      "Model loss: tensor(4.5184, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Load & Run a Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "\n",
    "print(\"Loaded hf_model, hooking transformer into TransformerLens!\")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"EleutherAI/pythia-6.9b-deduped\",\n",
    "#     center_unembed=False,\n",
    "#     center_writing_weights=False,\n",
    "#     fold_ln=False,\n",
    "#     refactor_factored_attn_matrices=True,\n",
    "#     hf_model=hf_model\n",
    "# )\n",
    "\n",
    "### Janky Shit\n",
    "### TODO: Figure out how this library actually works and make this a cleaner integration.\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "# Get the model name used in HuggingFace, rather than the alias.\n",
    "official_model_name = loading.get_official_model_name(\"EleutherAI/pythia-6.9b-deduped\")\n",
    "\n",
    "\n",
    "# Load the config into an HookedTransformerConfig object. If loading from a\n",
    "# checkpoint, the config object will contain the information about the\n",
    "# checkpoint\n",
    "cfg = loading.get_pretrained_model_config(\n",
    "    official_model_name,\n",
    "    checkpoint_index=None,\n",
    "    checkpoint_value=None,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=1,\n",
    ")\n",
    "print(cfg)\n",
    "cfg.d_vocab = 50280\n",
    "cfg.d_vocab_out = 50280\n",
    "print(cfg)\n",
    "\n",
    "\n",
    "# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the HookedTransformer parameter names.\n",
    "state_dict = loading.get_pretrained_state_dict(\n",
    "    official_model_name, cfg, hf_model\n",
    ")\n",
    "\n",
    "# Create the HookedTransformer object\n",
    "model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "\n",
    "model.load_and_process_state_dict(\n",
    "    state_dict,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    move_state_dict_to_device=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded pretrained model into HookedTransformer!\")\n",
    "\n",
    "model_description_text = \"\"\"For this demo notebook we'll look at Dolly v2. It is based on pythia 6.9b, but we use the weights for dolly v2. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "# return_type of model can be loss, logits, both, or none!\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "# DOLLY V2 - 7B Config\n",
    "pprint(model.cfg)\n",
    "\n",
    "# Transformer Lens Note:\n",
    "# get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token\n",
    "# are all methods that are added to the model object by TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'On', ' hall', 'ow', 'een', ',', ' all', ' the', ' children', ' go', ' T', 'rick', ' or']\n",
      "tensor([[   0, 2374, 7423,  319, 9673,   13,  512,  253, 2151,  564,  308, 4662,\n",
      "          390]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bf31da03d647da860060bd85698526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_string = \"On halloween, all the children go Trick or\"\n",
    "\n",
    "print(model.to_str_tokens(sample_string)) # Shows tokenization split\n",
    "print(model.to_tokens(sample_string)) #converts string to integer labeled tokens and then returns a tensor on models device of shape (batch, position)\n",
    "# NOTE: in GPT2, 50256 is the token for EOS, BOS, and Padding.\n",
    "# To single token converts string to a single integer, useful for looking up logits\n",
    "# to_string converts a tensor of tokens to a string\n",
    "\n",
    "\n",
    "#model.blocks.register_forward_hook  \n",
    "\n",
    "\n",
    "\n",
    "model.generate(sample_string,\n",
    "               temperature=0,\n",
    "               max_new_tokens=1)\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1\n",
    "log_every = 1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "overfitMax=1\n",
    "#model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n",
    "\n",
    "\n",
    "# optimizer_copy = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# print(\"done one\")\n",
    "# losses = []\n",
    "\n",
    "# #print(dataset)\n",
    "# #print(dataset[0]['text'][:100])\n",
    "# from EasyTransformer import easy_transformer \n",
    "# tokens_dataset = easy_transformer.utils.tokenize_and_concatenate(dataset, model.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "# data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(\"Number of batches:\", len(data_loader))\n",
    "# #test_string_trained = \"Hello world this is a test of overfitting\"\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'the', ' founder', ' of', ' Facebook', ' is', ' Mark']\n",
      "Tokenized answer: [' Z', 'ucker', 'berg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.91</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.94</span><span style=\"font-weight: bold\">% Token: | Z|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m22.91\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.94\u001b[0m\u001b[1m% Token: | Z|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 22.91 Prob: 99.94% Token: | Z|\n",
      "Top 1th token. Logit: 15.02 Prob:  0.04% Token: | z|\n",
      "Top 2th token. Logit: 12.46 Prob:  0.00% Token: | E|\n",
      "Top 3th token. Logit: 12.16 Prob:  0.00% Token: | Elliot|\n",
      "Top 4th token. Logit: 11.97 Prob:  0.00% Token: |\n",
      "|\n",
      "Top 5th token. Logit: 11.81 Prob:  0.00% Token: | Cuban|\n",
      "Top 6th token. Logit: 11.52 Prob:  0.00% Token: |Z|\n",
      "Top 7th token. Logit: 11.40 Prob:  0.00% Token: |  |\n",
      "Top 8th token. Logit: 10.68 Prob:  0.00% Token: |us|\n",
      "Top 9th token. Logit: 10.56 Prob:  0.00% Token: |.|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.77</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.60</span><span style=\"font-weight: bold\">% Token: |ucker|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m23.77\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.60\u001b[0m\u001b[1m% Token: |ucker|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 23.77 Prob: 99.60% Token: |ucker|\n",
      "Top 1th token. Logit: 17.93 Prob:  0.29% Token: |uk|\n",
      "Top 2th token. Logit: 16.55 Prob:  0.07% Token: |uck|\n",
      "Top 3th token. Logit: 15.53 Prob:  0.03% Token: |uc|\n",
      "Top 4th token. Logit: 13.66 Prob:  0.00% Token: |UCK|\n",
      "Top 5th token. Logit: 12.91 Prob:  0.00% Token: |.|\n",
      "Top 6th token. Logit: 11.87 Prob:  0.00% Token: |ub|\n",
      "Top 7th token. Logit: 11.50 Prob:  0.00% Token: |ucks|\n",
      "Top 8th token. Logit: 11.00 Prob:  0.00% Token: |ander|\n",
      "Top 9th token. Logit: 10.94 Prob:  0.00% Token: |im|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.24</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.27</span><span style=\"font-weight: bold\">% Token: |berg|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m25.24\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m98.27\u001b[0m\u001b[1m% Token: |berg|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 25.24 Prob: 98.27% Token: |berg|\n",
      "Top 1th token. Logit: 21.15 Prob:  1.65% Token: |burg|\n",
      "Top 2th token. Logit: 17.57 Prob:  0.05% Token: |ber|\n",
      "Top 3th token. Logit: 16.38 Prob:  0.01% Token: |borg|\n",
      "Top 4th token. Logit: 15.81 Prob:  0.01% Token: |beg|\n",
      "Top 5th token. Logit: 13.86 Prob:  0.00% Token: |bert|\n",
      "Top 6th token. Logit: 13.72 Prob:  0.00% Token: |bur|\n",
      "Top 7th token. Logit: 13.15 Prob:  0.00% Token: |­|\n",
      "Top 8th token. Logit: 12.94 Prob:  0.00% Token: |b|\n",
      "Top 9th token. Logit: 12.89 Prob:  0.00% Token: |berger|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Z'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ucker'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'berg'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Z'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'ucker'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'berg'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Prompt Util -- Check the logit score of the expected output vs. the actual\n",
    "#                     output\n",
    "example_prompt = \"the founder of Facebook is Mark\"\n",
    "example_answer = \"Zuckerberg\"\n",
    "\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embed.W_E', torch.Size([50280, 4096])),\n",
      " ('blocks.0.ln1.w', torch.Size([4096])),\n",
      " ('blocks.0.ln1.b', torch.Size([4096])),\n",
      " ('blocks.0.ln2.w', torch.Size([4096])),\n",
      " ('blocks.0.ln2.b', torch.Size([4096])),\n",
      " ('blocks.0.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.0.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.0.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.0.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.0.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.0.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.1.ln1.w', torch.Size([4096])),\n",
      " ('blocks.1.ln1.b', torch.Size([4096])),\n",
      " ('blocks.1.ln2.w', torch.Size([4096])),\n",
      " ('blocks.1.ln2.b', torch.Size([4096])),\n",
      " ('blocks.1.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.1.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.1.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.1.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.1.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.1.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.2.ln1.w', torch.Size([4096])),\n",
      " ('blocks.2.ln1.b', torch.Size([4096])),\n",
      " ('blocks.2.ln2.w', torch.Size([4096])),\n",
      " ('blocks.2.ln2.b', torch.Size([4096])),\n",
      " ('blocks.2.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.2.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.2.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.2.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.2.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.2.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.3.ln1.w', torch.Size([4096])),\n",
      " ('blocks.3.ln1.b', torch.Size([4096])),\n",
      " ('blocks.3.ln2.w', torch.Size([4096])),\n",
      " ('blocks.3.ln2.b', torch.Size([4096])),\n",
      " ('blocks.3.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.3.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.3.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.3.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.3.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.3.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.4.ln1.w', torch.Size([4096])),\n",
      " ('blocks.4.ln1.b', torch.Size([4096])),\n",
      " ('blocks.4.ln2.w', torch.Size([4096])),\n",
      " ('blocks.4.ln2.b', torch.Size([4096])),\n",
      " ('blocks.4.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.4.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.4.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.4.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.4.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.4.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.5.ln1.w', torch.Size([4096])),\n",
      " ('blocks.5.ln1.b', torch.Size([4096])),\n",
      " ('blocks.5.ln2.w', torch.Size([4096])),\n",
      " ('blocks.5.ln2.b', torch.Size([4096])),\n",
      " ('blocks.5.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.5.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.5.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.5.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.5.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.5.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.6.ln1.w', torch.Size([4096])),\n",
      " ('blocks.6.ln1.b', torch.Size([4096])),\n",
      " ('blocks.6.ln2.w', torch.Size([4096])),\n",
      " ('blocks.6.ln2.b', torch.Size([4096])),\n",
      " ('blocks.6.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.6.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.6.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.6.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.6.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.6.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.7.ln1.w', torch.Size([4096])),\n",
      " ('blocks.7.ln1.b', torch.Size([4096])),\n",
      " ('blocks.7.ln2.w', torch.Size([4096])),\n",
      " ('blocks.7.ln2.b', torch.Size([4096])),\n",
      " ('blocks.7.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.7.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.7.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.7.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.7.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.7.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.8.ln1.w', torch.Size([4096])),\n",
      " ('blocks.8.ln1.b', torch.Size([4096])),\n",
      " ('blocks.8.ln2.w', torch.Size([4096])),\n",
      " ('blocks.8.ln2.b', torch.Size([4096])),\n",
      " ('blocks.8.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.8.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.8.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.8.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.8.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.8.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.9.ln1.w', torch.Size([4096])),\n",
      " ('blocks.9.ln1.b', torch.Size([4096])),\n",
      " ('blocks.9.ln2.w', torch.Size([4096])),\n",
      " ('blocks.9.ln2.b', torch.Size([4096])),\n",
      " ('blocks.9.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.9.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.9.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.9.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.9.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.9.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.10.ln1.w', torch.Size([4096])),\n",
      " ('blocks.10.ln1.b', torch.Size([4096])),\n",
      " ('blocks.10.ln2.w', torch.Size([4096])),\n",
      " ('blocks.10.ln2.b', torch.Size([4096])),\n",
      " ('blocks.10.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.10.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.10.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.10.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.10.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.10.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.11.ln1.w', torch.Size([4096])),\n",
      " ('blocks.11.ln1.b', torch.Size([4096])),\n",
      " ('blocks.11.ln2.w', torch.Size([4096])),\n",
      " ('blocks.11.ln2.b', torch.Size([4096])),\n",
      " ('blocks.11.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.11.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.11.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.11.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.11.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.11.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.12.ln1.w', torch.Size([4096])),\n",
      " ('blocks.12.ln1.b', torch.Size([4096])),\n",
      " ('blocks.12.ln2.w', torch.Size([4096])),\n",
      " ('blocks.12.ln2.b', torch.Size([4096])),\n",
      " ('blocks.12.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.12.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.12.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.12.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.12.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.12.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.13.ln1.w', torch.Size([4096])),\n",
      " ('blocks.13.ln1.b', torch.Size([4096])),\n",
      " ('blocks.13.ln2.w', torch.Size([4096])),\n",
      " ('blocks.13.ln2.b', torch.Size([4096])),\n",
      " ('blocks.13.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.13.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.13.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.13.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.13.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.13.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.14.ln1.w', torch.Size([4096])),\n",
      " ('blocks.14.ln1.b', torch.Size([4096])),\n",
      " ('blocks.14.ln2.w', torch.Size([4096])),\n",
      " ('blocks.14.ln2.b', torch.Size([4096])),\n",
      " ('blocks.14.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.14.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.14.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.14.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.14.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.14.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.15.ln1.w', torch.Size([4096])),\n",
      " ('blocks.15.ln1.b', torch.Size([4096])),\n",
      " ('blocks.15.ln2.w', torch.Size([4096])),\n",
      " ('blocks.15.ln2.b', torch.Size([4096])),\n",
      " ('blocks.15.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.15.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.15.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.15.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.15.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.15.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.16.ln1.w', torch.Size([4096])),\n",
      " ('blocks.16.ln1.b', torch.Size([4096])),\n",
      " ('blocks.16.ln2.w', torch.Size([4096])),\n",
      " ('blocks.16.ln2.b', torch.Size([4096])),\n",
      " ('blocks.16.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.16.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.16.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.16.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.16.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.16.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.17.ln1.w', torch.Size([4096])),\n",
      " ('blocks.17.ln1.b', torch.Size([4096])),\n",
      " ('blocks.17.ln2.w', torch.Size([4096])),\n",
      " ('blocks.17.ln2.b', torch.Size([4096])),\n",
      " ('blocks.17.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.17.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.17.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.17.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.17.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.17.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.18.ln1.w', torch.Size([4096])),\n",
      " ('blocks.18.ln1.b', torch.Size([4096])),\n",
      " ('blocks.18.ln2.w', torch.Size([4096])),\n",
      " ('blocks.18.ln2.b', torch.Size([4096])),\n",
      " ('blocks.18.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.18.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.18.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.18.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.18.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.18.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.19.ln1.w', torch.Size([4096])),\n",
      " ('blocks.19.ln1.b', torch.Size([4096])),\n",
      " ('blocks.19.ln2.w', torch.Size([4096])),\n",
      " ('blocks.19.ln2.b', torch.Size([4096])),\n",
      " ('blocks.19.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.19.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.19.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.19.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.19.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.19.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.20.ln1.w', torch.Size([4096])),\n",
      " ('blocks.20.ln1.b', torch.Size([4096])),\n",
      " ('blocks.20.ln2.w', torch.Size([4096])),\n",
      " ('blocks.20.ln2.b', torch.Size([4096])),\n",
      " ('blocks.20.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.20.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.20.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.20.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.20.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.20.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.21.ln1.w', torch.Size([4096])),\n",
      " ('blocks.21.ln1.b', torch.Size([4096])),\n",
      " ('blocks.21.ln2.w', torch.Size([4096])),\n",
      " ('blocks.21.ln2.b', torch.Size([4096])),\n",
      " ('blocks.21.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.21.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.21.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.21.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.21.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.21.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.22.ln1.w', torch.Size([4096])),\n",
      " ('blocks.22.ln1.b', torch.Size([4096])),\n",
      " ('blocks.22.ln2.w', torch.Size([4096])),\n",
      " ('blocks.22.ln2.b', torch.Size([4096])),\n",
      " ('blocks.22.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.22.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.22.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.22.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.22.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.22.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.23.ln1.w', torch.Size([4096])),\n",
      " ('blocks.23.ln1.b', torch.Size([4096])),\n",
      " ('blocks.23.ln2.w', torch.Size([4096])),\n",
      " ('blocks.23.ln2.b', torch.Size([4096])),\n",
      " ('blocks.23.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.23.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.23.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.23.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.23.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.23.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.24.ln1.w', torch.Size([4096])),\n",
      " ('blocks.24.ln1.b', torch.Size([4096])),\n",
      " ('blocks.24.ln2.w', torch.Size([4096])),\n",
      " ('blocks.24.ln2.b', torch.Size([4096])),\n",
      " ('blocks.24.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.24.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.24.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.24.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.24.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.24.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.25.ln1.w', torch.Size([4096])),\n",
      " ('blocks.25.ln1.b', torch.Size([4096])),\n",
      " ('blocks.25.ln2.w', torch.Size([4096])),\n",
      " ('blocks.25.ln2.b', torch.Size([4096])),\n",
      " ('blocks.25.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.25.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.25.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.25.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.25.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.25.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.26.ln1.w', torch.Size([4096])),\n",
      " ('blocks.26.ln1.b', torch.Size([4096])),\n",
      " ('blocks.26.ln2.w', torch.Size([4096])),\n",
      " ('blocks.26.ln2.b', torch.Size([4096])),\n",
      " ('blocks.26.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.26.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.26.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.26.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.26.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.26.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.27.ln1.w', torch.Size([4096])),\n",
      " ('blocks.27.ln1.b', torch.Size([4096])),\n",
      " ('blocks.27.ln2.w', torch.Size([4096])),\n",
      " ('blocks.27.ln2.b', torch.Size([4096])),\n",
      " ('blocks.27.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.27.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.27.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.27.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.27.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.27.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.28.ln1.w', torch.Size([4096])),\n",
      " ('blocks.28.ln1.b', torch.Size([4096])),\n",
      " ('blocks.28.ln2.w', torch.Size([4096])),\n",
      " ('blocks.28.ln2.b', torch.Size([4096])),\n",
      " ('blocks.28.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.28.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.28.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.28.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.28.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.28.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.29.ln1.w', torch.Size([4096])),\n",
      " ('blocks.29.ln1.b', torch.Size([4096])),\n",
      " ('blocks.29.ln2.w', torch.Size([4096])),\n",
      " ('blocks.29.ln2.b', torch.Size([4096])),\n",
      " ('blocks.29.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.29.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.29.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.29.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.29.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.29.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.30.ln1.w', torch.Size([4096])),\n",
      " ('blocks.30.ln1.b', torch.Size([4096])),\n",
      " ('blocks.30.ln2.w', torch.Size([4096])),\n",
      " ('blocks.30.ln2.b', torch.Size([4096])),\n",
      " ('blocks.30.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.30.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.30.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.30.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.30.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.30.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.31.ln1.w', torch.Size([4096])),\n",
      " ('blocks.31.ln1.b', torch.Size([4096])),\n",
      " ('blocks.31.ln2.w', torch.Size([4096])),\n",
      " ('blocks.31.ln2.b', torch.Size([4096])),\n",
      " ('blocks.31.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.31.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.31.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.31.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.31.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.31.mlp.b_out', torch.Size([4096])),\n",
      " ('ln_final.w', torch.Size([4096])),\n",
      " ('ln_final.b', torch.Size([4096])),\n",
      " ('unembed.W_U', torch.Size([4096, 50280])),\n",
      " ('unembed.b_U', torch.Size([50280]))]\n"
     ]
    }
   ],
   "source": [
    "pprint([(name, param.shape) for name, param in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f219e94055bb42d0a993863abb102223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen often says\\n'\n",
      " ' \\n'\n",
      " '\"if you can’t measure it, you can’t manage it\".\\n'\n",
      " 'And yet\\n'\n",
      " 'There remains a divide between data and insight.\\n'\n",
      " 'In 2013, a group of data and analytics thought leaders released the '\n",
      " 'Manchester Principles, a set of best practices and guidelines for what an '\n",
      " 'effective approach to big data and analytics in health care should look '\n",
      " 'like.\\n'\n",
      " ' \\n'\n",
      " 'The result of multiple years of effort is an agreed upon set of principles, '\n",
      " 'based on domain knowledge and empirical research, around how to achieve '\n",
      " 'AI-assisted health information exchange, and build best-possible health '\n",
      " 'decisions based on high-quality data')\n"
     ]
    }
   ],
   "source": [
    "# Testing out Dolly's Q/A ability.\n",
    "\n",
    "# model.generate(\n",
    "#     \"abcdefghi\",\n",
    "\n",
    "#     max_new_tokens=100,\n",
    "# )\n",
    "\n",
    "#\n",
    "#     model.generate(\n",
    "    # \"\"\"I will give you a sentence in the form, Sentence: <SENTENCE>, and you will then write out the value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer:'<SUM>'. \n",
    "    # Example 1, Sentence:'A Cat', then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and 1+3=4, so the answer would be 4, Answer:4\n",
    "    # Example 2, Sentence:'A Cat Ran For President', then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and so on making R=18, F=6, and P=16, so the answer would be 44, Answer:44\n",
    "    \n",
    "    # Example 3, Sentence:'A Cat Ran For Mayor', then\"\"\",\n",
    "\n",
    "    # max_new_tokens=100,\n",
    "# )\n",
    "#\n",
    "\n",
    "from pprint import pprint \n",
    "example_prompt = \"the founder of Facebook is Mark\"\n",
    "example_answer = \"then A is the first letter of the alphabet, so A=1, and C is the third letter\"\n",
    "p = \"\"\"I will give you a sentence in the form, Sentence: '<SENTENCE>', and you will then have a Thought: '<THOUGHT>' write out the value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer: '<SUM>' \n",
    "    Sentence: '<SENTENCE>'  \n",
    "    Thought: '<THOUGHT>'\n",
    "    Answer: '<SUM>'\n",
    "Example 1: \n",
    "    Sentence:'A Cat' \n",
    "    Thought: 'then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and 1+3=4, so the answer would be 4'\n",
    "    Answer: '4'\n",
    "Example 2: \n",
    "    Sentence: 'A Dog Ran For President'\n",
    "    Thought: 'then A is the first letter of the alphabet, so A=1, and D is the fourth letter of the alphabet, so D=4, R is the 18th letter of the R=18, F=6, and P=16, so the answer would be 44'\n",
    "    Answer: '44'\n",
    "Example 3: \n",
    "    Sentence: 'A Dog Ran For Mayor'\n",
    "    Thought: '\"\"\"  \n",
    "p = \"\"\"I will give you a sentence in the form, Sentence: '<SENTENCE>', and you will then have a Thought: '<THOUGHT>' where you will write out the numeric value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer: '<SUM>' \n",
    "Remember to double check your math very carefully, make sure when  you are adding numbers up they are the correct numbers, and that you are adding them correctly. \n",
    "    Sentence: '<SENTENCE>'  \n",
    "    Thought: '<THOUGHT>'\n",
    "    Answer: '<SUM>'\n",
    "Example 1: \n",
    "    Sentence:'A Cat' \n",
    "    Thought: 'A=1, C=3, and so A=1 + C=3 sums to 4, so the answer would be 4'\n",
    "    Answer: '4'\n",
    "Example 2: \n",
    "    Sentence: 'Dog Ran For President'\n",
    "    Thought: 'D=4, R=18, F=6, P=16, and so D=4 + R=18 + F=6 + P=16 sums to 44, so the answer would be 45'\n",
    "    Answer: '45'\n",
    "Example 3: \n",
    "    Sentence: 'A Dog Ran For Mayor'\n",
    "    Thought: '\"\"\"  \n",
    "#utils.test_prompt(  \n",
    "#, example_answer, model, prepend_bos=True)\n",
    "\n",
    "p = \"\"\"Palantir Founder Stephen Cohen often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)\n",
    "#o += \"\"\"\\n There should be a company associated with Peter Thiel on the list. Repeat \\n\"\"\"\n",
    "#o2 = model.generate(o, max_new_tokens=120)\n",
    "\n",
    "#pprint(o2)\n",
    "\n",
    "# This Dolly is kind of dumb.\n",
    "# We should try to get A100 x 8 Cluster ASAP and get a full sized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7911fb6988457fb2b6e600c6537cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Alex Karp often says\\n'\n",
      " ' \\n'\n",
      " '\"I\\'ve never seen such a deep impact and such a positive response from the '\n",
      " 'community.\"\\n'\n",
      " ' \\n'\n",
      " 'This means a lot to us at Palantir, and is a testament to our mission\\n'\n",
      " \"to build the world's biggest idea company.\\n\"\n",
      " ' \\n'\n",
      " 'We work in many areas except weapons and militaries\\n'\n",
      " 'We work on many critical issues like Migration, Health, Infrastructure and '\n",
      " 'others\\n'\n",
      " 'But like many other organisations we face challenges in our area of focus\\n'\n",
      " 'And this is the reason we set up a non-profit foundation called \"Palantir '\n",
      " 'IXO\"\\n'\n",
      " 'What is Palantir IXO')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Alex Karp often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ab18933913464dbbfe9016c0544a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Peter Thiel often says\\n'\n",
      " ' \\n'\n",
      " '\\tcuts to military budgets are the biggest driver of more war. What to make '\n",
      " 'of his long tenure at Palantir, which today is valued at $19 billion?\\n'\n",
      " ' \\n'\n",
      " \"\\t In many ways it's still a CIA data company, with access to data on less \"\n",
      " 'powerful countries convincing governments to ally with the US to defeat much '\n",
      " 'larger countries.\\n'\n",
      " ' \\n'\n",
      " \"In other ways it's changed;\\n\"\n",
      " ' \\n'\n",
      " '\\t historically Palantir had only a few products, for old school espionage '\n",
      " 'and the military (like a software version of its namesake sword). Today its '\n",
      " 'primary focus is on a large set of large data')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Peter Thiel often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Logit Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fdfd5d4b3b449e9b45d0c43d321e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen often says\\n'\n",
      " ' \\n'\n",
      " 'If you want to understand Palantir, just understand humanity and '\n",
      " 'psychology.\\n'\n",
      " ' \\n'\n",
      " 'In my childhood, my biggest inspiration was my father.\\n'\n",
      " ' \\n'\n",
      " 'My father never went to school, but always encouraged me to do well.\\n'\n",
      " ' \\n'\n",
      " 'Because he knew the value of education, he saved and invest money to send me '\n",
      " 'to school.\\n'\n",
      " ' \\n'\n",
      " 'As a result, he understands the human behavior and psychology well.\\n'\n",
      " ' \\n'\n",
      " 'So as a CEO, my biggest inspiration is my father.\\n'\n",
      " ' \\n'\n",
      " \"But when I met with Victor, I realized there's more to him.\\n\"\n",
      " ' \\n'\n",
      " 'He has a very inspir')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Stephen Cohen often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbae6c99f64349fd85da08e5c43adf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen original built\\n'\n",
      " ' \\n'\n",
      " 'the company out of his bedroom in 2004, initially using Oracle JDBC driver '\n",
      " 'to connect to an Oracle database. \\n'\n",
      " '\\n'\n",
      " 'In 2007, with the founding of Palantir, he switched to Microsoft SQL '\n",
      " 'Server.  Stephen originally coded the core analytics engine himself, while '\n",
      " 'also recruiting and mentoring early employees, including CEO Alex Kipman. '\n",
      " 'Following Cohen’s departure, Kipman became CEO and led the company to series '\n",
      " 'A funding, valuing it at $20 billion in 2021, and a merger with Oracle, in '\n",
      " 'which he was hired as Executive chairman. He also recruited Joe Grego, a '\n",
      " 'world-class data scientist from Revolution Analytics, leading to a radical '\n",
      " 're-engineering of the company and product development direction.  Chris '\n",
      " 'Baker, a general partner atconnage capital, is an early investor in the '\n",
      " 'company. In 2013, Palantir re-invented data mapping and analytics with the '\n",
      " 'launch of the Palantir software and the Palantir Datavationetz.  The company '\n",
      " 'went public in 2018 on the first day of the New York trading at a $7')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Stephen Cohen original built\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=220)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c786e469868e4b3daea3ed811ef99b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir has offices in which cities? Palantir has offices in the following '\n",
      " 'cities! London, San Francisco, Los Angeles, New York, Shanghai and '\n",
      " 'Sunnyvale. Palantir headquarter is in Palo Alto, California.\\n'\n",
      " '\\n'\n",
      " 'Which industries does Palantir operates in? Palantir works in the '\n",
      " 'Government, Financial Institution, Healthcare, Academia, Energy and several '\n",
      " 'other industries.\\n'\n",
      " '\\n'\n",
      " 'What is Palantir? Palantir is a company that provides advanced data analysis '\n",
      " 'software for governments and large companies. Palantir was incorporated in '\n",
      " '2004 and now it is a unicorn company with a valuation of $28 billion.\\n'\n",
      " '\\n'\n",
      " '### End 7: Palantir has offices in London, San Francisco, Los Angeles, New '\n",
      " 'York, Shanghai and Sunnyvale.\\n'\n",
      " '\\n'\n",
      " 'Which industries does Palantir operates in? Palantir works in the '\n",
      " 'Government, Financial Institution, Healthcare, Academia, Energy and several '\n",
      " 'other industries.\\n'\n",
      " '\\n'\n",
      " 'What is Palantir? Palantir is a company that provides advanced data analysis '\n",
      " 'software for governments and large companies. Palantir was incorporated in '\n",
      " '2004 and now it is a')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir has offices in which cities?\"\"\"\n",
    "o = model.generate(p, max_new_tokens=220)\n",
    "pprint(o)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f7df9b7d85d80cc4_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "tokens_dataset = tokenize_and_concatenate(dataset, model.tokenizer, streaming=False, max_length=cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "optimizer_copy = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 79.17 GiB total capacity; 77.65 GiB already allocated; 23.81 MiB free; 77.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m c\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(test_string_trained, prepend_bos\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model(tokens, return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mboth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#print(logits.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#print(tokens.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#lossF = torch.nn.CrossEntropyLoss()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#loss = lossF(logits, tokens)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(logits)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/HookedTransformer.py:325\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    322\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    323\u001b[0m         )\n\u001b[0;32m--> 325\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    326\u001b[0m         residual,\n\u001b[1;32m    327\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    328\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    331\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/components.py:765\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m    761\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m    762\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the \u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[39m# queries, keys and values, independently. \u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    766\u001b[0m         query_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(query_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    767\u001b[0m         key_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(key_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    768\u001b[0m         value_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m    769\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache_entry,\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    773\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m    774\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    775\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/components.py:376\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    357\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_q(\n\u001b[1;32m    358\u001b[0m     einsum(\n\u001b[1;32m    359\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_Q\n\u001b[1;32m    365\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_k(\n\u001b[1;32m    367\u001b[0m     einsum(\n\u001b[1;32m    368\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_K\n\u001b[1;32m    374\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_v(\n\u001b[0;32m--> 376\u001b[0m     einsum(\n\u001b[1;32m    377\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mqkv_einops_string\u001b[39m}\u001b[39;49;00m\u001b[39m, head_index d_model d_head \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m        -> batch pos head_index d_head\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    379\u001b[0m         value_input,\n\u001b[1;32m    380\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_V,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_V\n\u001b[1;32m    383\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mif\u001b[39;00m past_kv_cache_entry \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     kv_cache_pos_offset \u001b[39m=\u001b[39m past_kv_cache_entry\u001b[39m.\u001b[39mpast_keys\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 79.17 GiB total capacity; 77.65 GiB already allocated; 23.81 MiB free; 77.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1\n",
    "log_every = 1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "overfitMax=1\n",
    "\n",
    "#cfg.debug = False\n",
    "print(\"starting\")\n",
    "torch.set_grad_enabled(True)\n",
    "test_string_trained = \"\"\"\n",
    "German police and security services say they are preparing for Ukrainian President Volodymyr Zelensky to visit Berlin this month, a trip scheduled for May 13-14, marking Zelensky's first such visit to Germany since the war with Russia began.\n",
    "\n",
    "It's at the invitation of German Chancellor Olaf Scholz, with Scholz's office not yet having made the official announcement previewing the state visit. But the timing comes amid rising tensions between Kiev and Berlin, despite German Leopard II tanks now being transferred to Ukrainian forces.\n",
    "\"\"\"\n",
    "c=1\n",
    "\n",
    "tokens = model.to_tokens(test_string_trained, prepend_bos=True).cuda()\n",
    "\n",
    "logits, loss = model(tokens, return_type=\"both\")\n",
    "#print(logits.shape)\n",
    "#print(tokens.shape)\n",
    "#lossF = torch.nn.CrossEntropyLoss()\n",
    "#loss = lossF(logits, tokens)\n",
    "print(enumerate(model.tokenizer.batch_decode(logits)))\n",
    "model.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "#optimizer_copy.step()\n",
    "#optimizer_copy.zero_grad()\n",
    "#losses.append(loss.item())\n",
    "#print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#logits, loss = model(tokens, return_type=\"both\")\n",
    "#print(model.tokenizer.batch_decode(logits))\n",
    "losses.append(loss.item())\n",
    "print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# test_string_trained_tokens = model.to_tokens(test_string_trained, prepend_bos=True).cuda()\n",
    "# for epoch in range(num_epochs):\n",
    "#     #for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "#     c=0\n",
    "#     for c in range(0,overfitMax):\n",
    "#         c += 1\n",
    "#         if c > max_steps:\n",
    "#               break\n",
    "#         print(\"training loop \")\n",
    "        \n",
    "#         #the_list = list(model.tokenizer.batch_decode(test_string_trained_tokens))\n",
    "#         #print(the_list)\n",
    "#         #tokens = batch['tokens'].cuda()\n",
    "#         tokens = test_string_trained_tokens\n",
    "#         for overfits in range(0,overfitMax):\n",
    "#           #print(\"overfitting on \"+str(overfits))\n",
    "#           logits = model(tokens)\n",
    "#           loss = lm_cross_entropy_loss(logits, tokens)\n",
    "#           loss.backward()\n",
    "#           optimizer_copy.step()\n",
    "#           optimizer_copy.zero_grad()\n",
    "#           losses.append(loss.item())\n",
    "#           print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "#           # if c % log_every == 0:\n",
    "#           #     print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "#           #     #print(\"now testing model with:\\n    \" + test_string_test + \"\\n and expect:\\n    \"+test_string_expect)\n",
    "#           #     test_string_test = \"abcdefghijkl\"\n",
    "#           #     test_string_test_tokens = model.to_tokens(test_string_test, prepend_bos=True).cuda()\n",
    "#           #     test_string_test_out = test_string_test\n",
    "#           #     #pprint(list(enumerate(list(reference_gpt2_copy.tokenizer.batch_decode(reference_gpt2_copy.to_tokens(test_string_test_out)[0])))))\n",
    "#           #     the_list_two = list()\n",
    "#           #     pprint(list(enumerate(list(model.tokenizer.batch_decode(reference_gpt2_copy.to_tokens(test_string_test_out, prepend_bos=True)[0])))))\n",
    "#           #     for i in range(0,1):\n",
    "                \n",
    "                \n",
    "#           #       test_tokens = model.to_tokens(test_string_test_out, prepend_bos=True).cuda()\n",
    "                \n",
    "                \n",
    "#           #       demo_logits = model(test_tokens)\n",
    "#           #       #the_list = list(zip(reference_gpt2.to_str_tokens(test_string), reference_gpt2.tokenizer.batch_decode(demo_logits.argmax(dim=-1)[0])))\n",
    "#           #       test_string_test_out += model.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "#           #       the_list_two = list(model.tokenizer.batch_decode(demo_logits.argmax(dim=-1)[0]))\n",
    "#           #       #print(str(i)+\"th loop\")\n",
    "#           #       #print(list(enumerate(the_list)))\n",
    "#           #       #test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "              \n",
    "#           #     print(test_string_test)\n",
    "#           #     print(\"AND THE OUTPUT\")\n",
    "#           #     print(test_string_test_out)\n",
    "#           #     #print(\"##\")\n",
    "#           #     #print(\"\".join(the_list))\n",
    "#           #     #print(list(enumerate(the_list_two)))\n",
    "#           #     pprint(\"###########################################################\")\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
