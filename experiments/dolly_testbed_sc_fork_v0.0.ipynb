{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Working with Dolly\n",
    "## Last Updated $DATE -  $AUTHOR.\n",
    "\n",
    "```\n",
    "Summary of High Level Research Question\n",
    "```\n",
    "\n",
    "Try to scope your experiments such you can answer your research question in 1-3 hours.\n",
    "This is an ideal time block to enter flow / deep work, but short enough that you will still feel \n",
    "motivated by a relatively tight feedback loop.\n",
    "\n",
    "If a problem seems like it needs more time that that, \n",
    "\n",
    "### High Level Experiment Design\n",
    "\n",
    "## Goals:\n",
    "```\n",
    "List of specific goals that this experiment seeks to achieve.\n",
    "\n",
    "This should fall under a few categories:\n",
    "- Development of Intuition about a _specific_ topic\n",
    "- Novel Research or Insight that could lead to a publishable result\n",
    "- Meaningfully explore a topic which could lead to an improvement in product\n",
    "\n",
    "Guiding principles should understanding, insight, and value creation.\n",
    "```\n",
    "\n",
    "## Tasks & Experiment Design\n",
    "\n",
    "```\n",
    "A list of specific tasks that are going to be tested \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "```\n",
    "Document high level research findings and how\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
      "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-fn5so28x\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-fn5so28x\n",
      "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 0ffcc8ad647d9e991f4c2596557a9d7475617773\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.28.1)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (13.2.0)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (2.12.0)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.15.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.2.15)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.12.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (2.13.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.1.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.12.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.19.6)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.30)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (18.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2019.11.28)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.9/dist-packages (1.39.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.23.4)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.12.1+cu116)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (5.14.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly) (8.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install things into ENV\n",
    "# TODO: Setup up a container and push to docker that contains all these\n",
    "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "%pip install circuitsvis\n",
    "%pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Set of Imports for MI Research\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch configuration for inference based experiments\n",
    "# NOTE: Mark as False if you want to do any kind of training \n",
    "#       as part of your experimentation\n",
    "\n",
    "INFERENCE_ONLY_EXPERIMENT = True\n",
    "if INFERENCE_ONLY_EXPERIMENT:\n",
    "    torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-b603068d-8cf6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-b603068d-8cf6\",\n",
       "      Hello,\n",
       "      {\"name\": \"Vivek\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f0d69428f10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Circuit Visualizations\n",
    "# TODO: Explore building out our own packages / tooling\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Vivek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hf_model, hooking transformer into TransformerLens!\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50432,\n",
      " 'd_vocab_out': 50432,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "Loaded pretrained model into HookedTransformer!\n",
      "Model loss: tensor(4.5184, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Load & Run a Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "\n",
    "print(\"Loaded hf_model, hooking transformer into TransformerLens!\")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"EleutherAI/pythia-6.9b-deduped\",\n",
    "#     center_unembed=False,\n",
    "#     center_writing_weights=False,\n",
    "#     fold_ln=False,\n",
    "#     refactor_factored_attn_matrices=True,\n",
    "#     hf_model=hf_model\n",
    "# )\n",
    "\n",
    "### Janky Shit\n",
    "### TODO: Figure out how this library actually works and make this a cleaner integration.\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "# Get the model name used in HuggingFace, rather than the alias.\n",
    "official_model_name = loading.get_official_model_name(\"EleutherAI/pythia-6.9b-deduped\")\n",
    "\n",
    "\n",
    "# Load the config into an HookedTransformerConfig object. If loading from a\n",
    "# checkpoint, the config object will contain the information about the\n",
    "# checkpoint\n",
    "cfg = loading.get_pretrained_model_config(\n",
    "    official_model_name,\n",
    "    checkpoint_index=None,\n",
    "    checkpoint_value=None,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=1,\n",
    ")\n",
    "print(cfg)\n",
    "cfg.d_vocab = 50280\n",
    "cfg.d_vocab_out = 50280\n",
    "print(cfg)\n",
    "\n",
    "\n",
    "# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the HookedTransformer parameter names.\n",
    "state_dict = loading.get_pretrained_state_dict(\n",
    "    official_model_name, cfg, hf_model\n",
    ")\n",
    "\n",
    "# Create the HookedTransformer object\n",
    "model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "\n",
    "model.load_and_process_state_dict(\n",
    "    state_dict,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    move_state_dict_to_device=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded pretrained model into HookedTransformer!\")\n",
    "\n",
    "model_description_text = \"\"\"For this demo notebook we'll look at Dolly v2. It is based on pythia 6.9b, but we use the weights for dolly v2. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "# return_type of model can be loss, logits, both, or none!\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "# DOLLY V2 - 7B Config\n",
    "pprint(model.cfg)\n",
    "\n",
    "# Transformer Lens Note:\n",
    "# get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token\n",
    "# are all methods that are added to the model object by TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'On', ' hall', 'ow', 'een', ',', ' all', ' the', ' children', ' go', ' T', 'rick', ' or']\n",
      "tensor([[   0, 2374, 7423,  319, 9673,   13,  512,  253, 2151,  564,  308, 4662,\n",
      "          390]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede3a6d86b5448ccbcacfc4f215fde4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_string = \"On halloween, all the children go Trick or\"\n",
    "\n",
    "print(model.to_str_tokens(sample_string)) # Shows tokenization split\n",
    "print(model.to_tokens(sample_string)) #converts string to integer labeled tokens and then returns a tensor on models device of shape (batch, position)\n",
    "# NOTE: in GPT2, 50256 is the token for EOS, BOS, and Padding.\n",
    "# To single token converts string to a single integer, useful for looking up logits\n",
    "# to_string converts a tensor of tokens to a string\n",
    "\n",
    "\n",
    "#model.blocks.register_forward_hook  \n",
    "\n",
    "\n",
    "\n",
    "model.generate(sample_string,\n",
    "               temperature=0,\n",
    "               max_new_tokens=1)\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1\n",
    "log_every = 1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "overfitMax=1\n",
    "#model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n",
    "\n",
    "\n",
    "# optimizer_copy = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# print(\"done one\")\n",
    "# losses = []\n",
    "\n",
    "# #print(dataset)\n",
    "# #print(dataset[0]['text'][:100])\n",
    "# from EasyTransformer import easy_transformer \n",
    "# tokens_dataset = easy_transformer.utils.tokenize_and_concatenate(dataset, model.tokenizer, streaming=False, max_length=model_cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "# data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "# print(\"Number of batches:\", len(data_loader))\n",
    "# #test_string_trained = \"Hello world this is a test of overfitting\"\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'the', ' founder', ' of', ' Facebook', ' is', ' Mark']\n",
      "Tokenized answer: [' Z', 'ucker', 'berg']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.91</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.94</span><span style=\"font-weight: bold\">% Token: | Z|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m22.91\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.94\u001b[0m\u001b[1m% Token: | Z|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 22.91 Prob: 99.94% Token: | Z|\n",
      "Top 1th token. Logit: 15.02 Prob:  0.04% Token: | z|\n",
      "Top 2th token. Logit: 12.46 Prob:  0.00% Token: | E|\n",
      "Top 3th token. Logit: 12.16 Prob:  0.00% Token: | Elliot|\n",
      "Top 4th token. Logit: 11.97 Prob:  0.00% Token: |\n",
      "|\n",
      "Top 5th token. Logit: 11.81 Prob:  0.00% Token: | Cuban|\n",
      "Top 6th token. Logit: 11.52 Prob:  0.00% Token: |Z|\n",
      "Top 7th token. Logit: 11.40 Prob:  0.00% Token: |  |\n",
      "Top 8th token. Logit: 10.68 Prob:  0.00% Token: |us|\n",
      "Top 9th token. Logit: 10.56 Prob:  0.00% Token: |.|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.77</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.60</span><span style=\"font-weight: bold\">% Token: |ucker|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m23.77\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m99.60\u001b[0m\u001b[1m% Token: |ucker|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 23.77 Prob: 99.60% Token: |ucker|\n",
      "Top 1th token. Logit: 17.93 Prob:  0.29% Token: |uk|\n",
      "Top 2th token. Logit: 16.55 Prob:  0.07% Token: |uck|\n",
      "Top 3th token. Logit: 15.53 Prob:  0.03% Token: |uc|\n",
      "Top 4th token. Logit: 13.66 Prob:  0.00% Token: |UCK|\n",
      "Top 5th token. Logit: 12.91 Prob:  0.00% Token: |.|\n",
      "Top 6th token. Logit: 11.87 Prob:  0.00% Token: |ub|\n",
      "Top 7th token. Logit: 11.50 Prob:  0.00% Token: |ucks|\n",
      "Top 8th token. Logit: 11.00 Prob:  0.00% Token: |ander|\n",
      "Top 9th token. Logit: 10.94 Prob:  0.00% Token: |im|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.24</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.27</span><span style=\"font-weight: bold\">% Token: |berg|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m25.24\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m98.27\u001b[0m\u001b[1m% Token: |berg|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 25.24 Prob: 98.27% Token: |berg|\n",
      "Top 1th token. Logit: 21.15 Prob:  1.65% Token: |burg|\n",
      "Top 2th token. Logit: 17.57 Prob:  0.05% Token: |ber|\n",
      "Top 3th token. Logit: 16.38 Prob:  0.01% Token: |borg|\n",
      "Top 4th token. Logit: 15.81 Prob:  0.01% Token: |beg|\n",
      "Top 5th token. Logit: 13.86 Prob:  0.00% Token: |bert|\n",
      "Top 6th token. Logit: 13.72 Prob:  0.00% Token: |bur|\n",
      "Top 7th token. Logit: 13.15 Prob:  0.00% Token: |Â­|\n",
      "Top 8th token. Logit: 12.94 Prob:  0.00% Token: |b|\n",
      "Top 9th token. Logit: 12.89 Prob:  0.00% Token: |berger|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Z'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ucker'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>, <span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'berg'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Z'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'ucker'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m, \u001b[1m(\u001b[0m\u001b[32m'berg'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Prompt Util -- Check the logit score of the expected output vs. the actual\n",
    "#                     output\n",
    "example_prompt = \"the founder of Facebook is Mark\"\n",
    "example_answer = \"Zuckerberg\"\n",
    "\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('embed.W_E', torch.Size([50280, 4096])),\n",
      " ('blocks.0.ln1.w', torch.Size([4096])),\n",
      " ('blocks.0.ln1.b', torch.Size([4096])),\n",
      " ('blocks.0.ln2.w', torch.Size([4096])),\n",
      " ('blocks.0.ln2.b', torch.Size([4096])),\n",
      " ('blocks.0.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.0.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.0.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.0.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.0.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.0.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.0.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.0.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.1.ln1.w', torch.Size([4096])),\n",
      " ('blocks.1.ln1.b', torch.Size([4096])),\n",
      " ('blocks.1.ln2.w', torch.Size([4096])),\n",
      " ('blocks.1.ln2.b', torch.Size([4096])),\n",
      " ('blocks.1.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.1.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.1.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.1.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.1.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.1.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.1.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.1.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.2.ln1.w', torch.Size([4096])),\n",
      " ('blocks.2.ln1.b', torch.Size([4096])),\n",
      " ('blocks.2.ln2.w', torch.Size([4096])),\n",
      " ('blocks.2.ln2.b', torch.Size([4096])),\n",
      " ('blocks.2.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.2.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.2.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.2.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.2.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.2.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.2.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.2.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.3.ln1.w', torch.Size([4096])),\n",
      " ('blocks.3.ln1.b', torch.Size([4096])),\n",
      " ('blocks.3.ln2.w', torch.Size([4096])),\n",
      " ('blocks.3.ln2.b', torch.Size([4096])),\n",
      " ('blocks.3.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.3.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.3.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.3.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.3.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.3.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.3.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.3.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.4.ln1.w', torch.Size([4096])),\n",
      " ('blocks.4.ln1.b', torch.Size([4096])),\n",
      " ('blocks.4.ln2.w', torch.Size([4096])),\n",
      " ('blocks.4.ln2.b', torch.Size([4096])),\n",
      " ('blocks.4.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.4.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.4.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.4.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.4.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.4.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.4.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.4.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.5.ln1.w', torch.Size([4096])),\n",
      " ('blocks.5.ln1.b', torch.Size([4096])),\n",
      " ('blocks.5.ln2.w', torch.Size([4096])),\n",
      " ('blocks.5.ln2.b', torch.Size([4096])),\n",
      " ('blocks.5.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.5.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.5.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.5.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.5.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.5.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.5.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.5.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.6.ln1.w', torch.Size([4096])),\n",
      " ('blocks.6.ln1.b', torch.Size([4096])),\n",
      " ('blocks.6.ln2.w', torch.Size([4096])),\n",
      " ('blocks.6.ln2.b', torch.Size([4096])),\n",
      " ('blocks.6.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.6.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.6.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.6.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.6.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.6.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.6.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.6.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.7.ln1.w', torch.Size([4096])),\n",
      " ('blocks.7.ln1.b', torch.Size([4096])),\n",
      " ('blocks.7.ln2.w', torch.Size([4096])),\n",
      " ('blocks.7.ln2.b', torch.Size([4096])),\n",
      " ('blocks.7.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.7.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.7.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.7.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.7.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.7.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.7.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.7.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.8.ln1.w', torch.Size([4096])),\n",
      " ('blocks.8.ln1.b', torch.Size([4096])),\n",
      " ('blocks.8.ln2.w', torch.Size([4096])),\n",
      " ('blocks.8.ln2.b', torch.Size([4096])),\n",
      " ('blocks.8.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.8.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.8.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.8.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.8.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.8.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.8.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.8.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.9.ln1.w', torch.Size([4096])),\n",
      " ('blocks.9.ln1.b', torch.Size([4096])),\n",
      " ('blocks.9.ln2.w', torch.Size([4096])),\n",
      " ('blocks.9.ln2.b', torch.Size([4096])),\n",
      " ('blocks.9.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.9.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.9.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.9.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.9.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.9.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.9.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.9.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.10.ln1.w', torch.Size([4096])),\n",
      " ('blocks.10.ln1.b', torch.Size([4096])),\n",
      " ('blocks.10.ln2.w', torch.Size([4096])),\n",
      " ('blocks.10.ln2.b', torch.Size([4096])),\n",
      " ('blocks.10.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.10.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.10.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.10.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.10.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.10.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.10.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.10.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.11.ln1.w', torch.Size([4096])),\n",
      " ('blocks.11.ln1.b', torch.Size([4096])),\n",
      " ('blocks.11.ln2.w', torch.Size([4096])),\n",
      " ('blocks.11.ln2.b', torch.Size([4096])),\n",
      " ('blocks.11.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.11.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.11.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.11.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.11.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.11.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.11.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.11.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.12.ln1.w', torch.Size([4096])),\n",
      " ('blocks.12.ln1.b', torch.Size([4096])),\n",
      " ('blocks.12.ln2.w', torch.Size([4096])),\n",
      " ('blocks.12.ln2.b', torch.Size([4096])),\n",
      " ('blocks.12.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.12.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.12.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.12.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.12.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.12.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.12.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.12.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.13.ln1.w', torch.Size([4096])),\n",
      " ('blocks.13.ln1.b', torch.Size([4096])),\n",
      " ('blocks.13.ln2.w', torch.Size([4096])),\n",
      " ('blocks.13.ln2.b', torch.Size([4096])),\n",
      " ('blocks.13.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.13.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.13.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.13.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.13.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.13.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.13.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.13.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.14.ln1.w', torch.Size([4096])),\n",
      " ('blocks.14.ln1.b', torch.Size([4096])),\n",
      " ('blocks.14.ln2.w', torch.Size([4096])),\n",
      " ('blocks.14.ln2.b', torch.Size([4096])),\n",
      " ('blocks.14.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.14.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.14.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.14.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.14.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.14.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.14.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.14.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.15.ln1.w', torch.Size([4096])),\n",
      " ('blocks.15.ln1.b', torch.Size([4096])),\n",
      " ('blocks.15.ln2.w', torch.Size([4096])),\n",
      " ('blocks.15.ln2.b', torch.Size([4096])),\n",
      " ('blocks.15.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.15.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.15.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.15.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.15.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.15.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.15.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.15.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.16.ln1.w', torch.Size([4096])),\n",
      " ('blocks.16.ln1.b', torch.Size([4096])),\n",
      " ('blocks.16.ln2.w', torch.Size([4096])),\n",
      " ('blocks.16.ln2.b', torch.Size([4096])),\n",
      " ('blocks.16.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.16.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.16.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.16.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.16.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.16.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.16.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.16.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.17.ln1.w', torch.Size([4096])),\n",
      " ('blocks.17.ln1.b', torch.Size([4096])),\n",
      " ('blocks.17.ln2.w', torch.Size([4096])),\n",
      " ('blocks.17.ln2.b', torch.Size([4096])),\n",
      " ('blocks.17.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.17.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.17.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.17.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.17.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.17.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.17.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.17.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.18.ln1.w', torch.Size([4096])),\n",
      " ('blocks.18.ln1.b', torch.Size([4096])),\n",
      " ('blocks.18.ln2.w', torch.Size([4096])),\n",
      " ('blocks.18.ln2.b', torch.Size([4096])),\n",
      " ('blocks.18.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.18.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.18.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.18.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.18.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.18.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.18.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.18.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.19.ln1.w', torch.Size([4096])),\n",
      " ('blocks.19.ln1.b', torch.Size([4096])),\n",
      " ('blocks.19.ln2.w', torch.Size([4096])),\n",
      " ('blocks.19.ln2.b', torch.Size([4096])),\n",
      " ('blocks.19.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.19.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.19.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.19.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.19.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.19.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.19.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.19.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.20.ln1.w', torch.Size([4096])),\n",
      " ('blocks.20.ln1.b', torch.Size([4096])),\n",
      " ('blocks.20.ln2.w', torch.Size([4096])),\n",
      " ('blocks.20.ln2.b', torch.Size([4096])),\n",
      " ('blocks.20.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.20.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.20.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.20.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.20.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.20.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.20.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.20.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.21.ln1.w', torch.Size([4096])),\n",
      " ('blocks.21.ln1.b', torch.Size([4096])),\n",
      " ('blocks.21.ln2.w', torch.Size([4096])),\n",
      " ('blocks.21.ln2.b', torch.Size([4096])),\n",
      " ('blocks.21.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.21.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.21.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.21.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.21.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.21.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.21.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.21.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.22.ln1.w', torch.Size([4096])),\n",
      " ('blocks.22.ln1.b', torch.Size([4096])),\n",
      " ('blocks.22.ln2.w', torch.Size([4096])),\n",
      " ('blocks.22.ln2.b', torch.Size([4096])),\n",
      " ('blocks.22.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.22.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.22.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.22.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.22.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.22.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.22.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.22.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.23.ln1.w', torch.Size([4096])),\n",
      " ('blocks.23.ln1.b', torch.Size([4096])),\n",
      " ('blocks.23.ln2.w', torch.Size([4096])),\n",
      " ('blocks.23.ln2.b', torch.Size([4096])),\n",
      " ('blocks.23.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.23.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.23.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.23.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.23.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.23.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.23.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.23.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.24.ln1.w', torch.Size([4096])),\n",
      " ('blocks.24.ln1.b', torch.Size([4096])),\n",
      " ('blocks.24.ln2.w', torch.Size([4096])),\n",
      " ('blocks.24.ln2.b', torch.Size([4096])),\n",
      " ('blocks.24.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.24.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.24.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.24.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.24.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.24.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.24.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.24.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.25.ln1.w', torch.Size([4096])),\n",
      " ('blocks.25.ln1.b', torch.Size([4096])),\n",
      " ('blocks.25.ln2.w', torch.Size([4096])),\n",
      " ('blocks.25.ln2.b', torch.Size([4096])),\n",
      " ('blocks.25.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.25.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.25.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.25.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.25.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.25.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.25.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.25.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.26.ln1.w', torch.Size([4096])),\n",
      " ('blocks.26.ln1.b', torch.Size([4096])),\n",
      " ('blocks.26.ln2.w', torch.Size([4096])),\n",
      " ('blocks.26.ln2.b', torch.Size([4096])),\n",
      " ('blocks.26.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.26.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.26.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.26.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.26.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.26.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.26.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.26.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.27.ln1.w', torch.Size([4096])),\n",
      " ('blocks.27.ln1.b', torch.Size([4096])),\n",
      " ('blocks.27.ln2.w', torch.Size([4096])),\n",
      " ('blocks.27.ln2.b', torch.Size([4096])),\n",
      " ('blocks.27.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.27.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.27.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.27.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.27.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.27.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.27.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.27.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.28.ln1.w', torch.Size([4096])),\n",
      " ('blocks.28.ln1.b', torch.Size([4096])),\n",
      " ('blocks.28.ln2.w', torch.Size([4096])),\n",
      " ('blocks.28.ln2.b', torch.Size([4096])),\n",
      " ('blocks.28.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.28.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.28.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.28.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.28.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.28.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.28.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.28.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.29.ln1.w', torch.Size([4096])),\n",
      " ('blocks.29.ln1.b', torch.Size([4096])),\n",
      " ('blocks.29.ln2.w', torch.Size([4096])),\n",
      " ('blocks.29.ln2.b', torch.Size([4096])),\n",
      " ('blocks.29.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.29.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.29.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.29.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.29.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.29.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.29.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.29.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.30.ln1.w', torch.Size([4096])),\n",
      " ('blocks.30.ln1.b', torch.Size([4096])),\n",
      " ('blocks.30.ln2.w', torch.Size([4096])),\n",
      " ('blocks.30.ln2.b', torch.Size([4096])),\n",
      " ('blocks.30.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.30.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.30.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.30.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.30.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.30.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.30.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.30.mlp.b_out', torch.Size([4096])),\n",
      " ('blocks.31.ln1.w', torch.Size([4096])),\n",
      " ('blocks.31.ln1.b', torch.Size([4096])),\n",
      " ('blocks.31.ln2.w', torch.Size([4096])),\n",
      " ('blocks.31.ln2.b', torch.Size([4096])),\n",
      " ('blocks.31.attn.W_Q', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_K', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_V', torch.Size([32, 4096, 128])),\n",
      " ('blocks.31.attn.W_O', torch.Size([32, 128, 4096])),\n",
      " ('blocks.31.attn.b_Q', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_K', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_V', torch.Size([32, 128])),\n",
      " ('blocks.31.attn.b_O', torch.Size([4096])),\n",
      " ('blocks.31.mlp.W_in', torch.Size([4096, 16384])),\n",
      " ('blocks.31.mlp.b_in', torch.Size([16384])),\n",
      " ('blocks.31.mlp.W_out', torch.Size([16384, 4096])),\n",
      " ('blocks.31.mlp.b_out', torch.Size([4096])),\n",
      " ('ln_final.w', torch.Size([4096])),\n",
      " ('ln_final.b', torch.Size([4096])),\n",
      " ('unembed.W_U', torch.Size([4096, 50280])),\n",
      " ('unembed.b_U', torch.Size([50280]))]\n"
     ]
    }
   ],
   "source": [
    "pprint([(name, param.shape) for name, param in model.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eb00e1766f44e78281b7739ca6f2cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen often says\\n'\n",
      " ' \\n'\n",
      " '\"The future is already here â it\\'s just not very evenly distributed.\"\\n'\n",
      " '\\n'\n",
      " 'Since the foundation of the company in 2004,\\n'\n",
      " ' \\n'\n",
      " 'its co-founders have been awarded over $90 million in funding, led the '\n",
      " \"company through two rounds of acquisitions, and it's used by government, \"\n",
      " 'industry, and academic clients worldwide.\\n'\n",
      " '\\n'\n",
      " 'Palantir was named one of the âTop 35 technologists to watchâ by Time in '\n",
      " '2016\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'It boasts of clients like the Army, ICE, and the Department of Defense, and '\n",
      " 'claims to offer a technological âedgeâ to government agencies.')\n"
     ]
    }
   ],
   "source": [
    "# Testing out Dolly's Q/A ability.\n",
    "\n",
    "# model.generate(\n",
    "#     \"abcdefghi\",\n",
    "\n",
    "#     max_new_tokens=100,\n",
    "# )\n",
    "\n",
    "#\n",
    "#     model.generate(\n",
    "    # \"\"\"I will give you a sentence in the form, Sentence: <SENTENCE>, and you will then write out the value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer:'<SUM>'. \n",
    "    # Example 1, Sentence:'A Cat', then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and 1+3=4, so the answer would be 4, Answer:4\n",
    "    # Example 2, Sentence:'A Cat Ran For President', then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and so on making R=18, F=6, and P=16, so the answer would be 44, Answer:44\n",
    "    \n",
    "    # Example 3, Sentence:'A Cat Ran For Mayor', then\"\"\",\n",
    "\n",
    "    # max_new_tokens=100,\n",
    "# )\n",
    "#\n",
    "\n",
    "from pprint import pprint \n",
    "example_prompt = \"the founder of Facebook is Mark\"\n",
    "example_answer = \"then A is the first letter of the alphabet, so A=1, and C is the third letter\"\n",
    "p = \"\"\"I will give you a sentence in the form, Sentence: '<SENTENCE>', and you will then have a Thought: '<THOUGHT>' write out the value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer: '<SUM>' \n",
    "    Sentence: '<SENTENCE>'  \n",
    "    Thought: '<THOUGHT>'\n",
    "    Answer: '<SUM>'\n",
    "Example 1: \n",
    "    Sentence:'A Cat' \n",
    "    Thought: 'then A is the first letter of the alphabet, so A=1, and C is the third letter, so C=3, and 1+3=4, so the answer would be 4'\n",
    "    Answer: '4'\n",
    "Example 2: \n",
    "    Sentence: 'A Dog Ran For President'\n",
    "    Thought: 'then A is the first letter of the alphabet, so A=1, and D is the fourth letter of the alphabet, so D=4, R is the 18th letter of the R=18, F=6, and P=16, so the answer would be 44'\n",
    "    Answer: '44'\n",
    "Example 3: \n",
    "    Sentence: 'A Dog Ran For Mayor'\n",
    "    Thought: '\"\"\"  \n",
    "p = \"\"\"I will give you a sentence in the form, Sentence: '<SENTENCE>', and you will then have a Thought: '<THOUGHT>' where you will write out the numeric value of the first letter of each word by converting each leading letter to a number, and then add all the numbers up to get their total sum and respond Answer: '<SUM>' \n",
    "Remember to double check your math very carefully, make sure when  you are adding numbers up they are the correct numbers, and that you are adding them correctly. \n",
    "    Sentence: '<SENTENCE>'  \n",
    "    Thought: '<THOUGHT>'\n",
    "    Answer: '<SUM>'\n",
    "Example 1: \n",
    "    Sentence:'A Cat' \n",
    "    Thought: 'A=1, C=3, and so A=1 + C=3 sums to 4, so the answer would be 4'\n",
    "    Answer: '4'\n",
    "Example 2: \n",
    "    Sentence: 'Dog Ran For President'\n",
    "    Thought: 'D=4, R=18, F=6, P=16, and so D=4 + R=18 + F=6 + P=16 sums to 44, so the answer would be 45'\n",
    "    Answer: '45'\n",
    "Example 3: \n",
    "    Sentence: 'A Dog Ran For Mayor'\n",
    "    Thought: '\"\"\"  \n",
    "#utils.test_prompt(  \n",
    "#, example_answer, model, prepend_bos=True)\n",
    "\n",
    "p = \"\"\"Palantir Founder Stephen Cohen often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)\n",
    "#o += \"\"\"\\n There should be a company associated with Peter Thiel on the list. Repeat \\n\"\"\"\n",
    "#o2 = model.generate(o, max_new_tokens=120)\n",
    "\n",
    "#pprint(o2)\n",
    "\n",
    "# This Dolly is kind of dumb.\n",
    "# We should try to get A100 x 8 Cluster ASAP and get a full sized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e9c4ae94e4cd1b9ef0c2c4f2755e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Alex Karp often says\\n'\n",
      " ' \\n'\n",
      " '\\n'\n",
      " ' 1. \"We ship quickly\".\\n'\n",
      " ' \\n'\n",
      " '\\n'\n",
      " '  2. \"We don\\'t build A.I. to sell to C.I.A. We build A.I. to sell to C3I '\n",
      " 'companies\".\\n'\n",
      " ' \\n'\n",
      " '\\n'\n",
      " \"  3.  We don't hire slow, we franchise.\\n\"\n",
      " ' \\n'\n",
      " \" 4.  We don't control our own destiny  - we collaborate with hundreds of \"\n",
      " 'customers to build the best experience.\\n'\n",
      " ' \\n'\n",
      " '5.  Spending money to create product market fit is the best economics.\\n'\n",
      " ' \\n'\n",
      " '6.  Our competitors often want to build a billion dollar company before they '\n",
      " 'get started')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Alex Karp often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9048dd25704558afd017d639e330a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Peter Thiel often says\\n'\n",
      " ' \\n'\n",
      " 'In his book Endgame he describes a fictional sci-fi setting in the 2020s '\n",
      " 'where advanced AI agents develop a pathological need for human attention and '\n",
      " 'contribution. If left unattended they become insane and attempt to take over '\n",
      " 'the world.\\n'\n",
      " ' \\n'\n",
      " 'According to Thiel, the setting is based on real history with insights into '\n",
      " 'how it could develop further. He names core techno-social technologies that '\n",
      " 'enable the scenario including Artificial General Intelligence, Collective '\n",
      " 'Intelligence, Enhanced Human Genomes, Virtual Reality and Smart Contracts.\\n'\n",
      " ' \\n'\n",
      " 'One solution to the problem he describes is creating an Internet of Thing')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Peter Thiel often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Logit Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefa5832d58447ecbed1af8abb8419a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen often says\\n'\n",
      " ' \\n'\n",
      " '\"warfare as we know it in the west will end, computer warfare will occur, '\n",
      " 'and no army will be able to compete.\"\\n'\n",
      " ' \\n'\n",
      " 'Palantir executive chair and co-founder, Peter Thiel, said in 2020\\n'\n",
      " ' \\n'\n",
      " '\"Stephen Cohen was telling me somebody just said that warfare as we know it '\n",
      " 'will end. Even though you can probably find exceptions, he said the general '\n",
      " \"sentiment is that if you're an elite unit and you step on the field of \"\n",
      " 'battle you have a 90 percent likelihood of losing. WAR will end\".\\n'\n",
      " ' \\n'\n",
      " 'See the full quote above.\\n'\n",
      " ' \\n')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Stephen Cohen often says\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=120)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951766d7631c4b79984844e8ba0fa61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir Founder Stephen Cohen often says\\n'\n",
      " ' \\n'\n",
      " '\"privacy is a threatened freedom, because the industry for monetizing your '\n",
      " \"data is growing so fast that it can't even measure all the data it \"\n",
      " 'generates\"\\n'\n",
      " ' \\n'\n",
      " 'in this talk he says\\n'\n",
      " '\"Privacy by Design\"\\n'\n",
      " ' \\n'\n",
      " 'may sound good on an elevator pitch\\n'\n",
      " 'but in the actual deep dive it just sounds like more buzz words\\n'\n",
      " ' \\n'\n",
      " 'privacy by design means create a system the works flawlessly the first time '\n",
      " 'without any bugs, with no security holes, across architectures and hardware '\n",
      " 'etc.  Cohen himself admits its really hard to actually do\\n'\n",
      " ' \\n'\n",
      " 'and even if it could be done\\n'\n",
      " 'the ethics and morality to show some of the data to an user can be sold to '\n",
      " 'improve the system is challenged by many including the CEO of Palantir who '\n",
      " 'funded the founding of palantir\\n'\n",
      " ' \\n'\n",
      " \"in reality Cohen's company Palantir sells the data it collects along with \"\n",
      " 'other user data to large companies like amazon and fortune 500 companies\\n'\n",
      " ' \\n'\n",
      " 'working on the other side of the duel use spectrum is \\n'\n",
      " 'aws likesle\\n'\n",
      " ' \\n'\n",
      " 'the company has build a platform to')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir Founder Stephen Cohen original built\n",
    " \"\"\"\n",
    "o = model.generate(p, max_new_tokens=220)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196df4b444f74d23adb933f01cc6be75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Palantir has offices in which cities?\\n'\n",
      " '\\n'\n",
      " 'Palantir has offices in which cities?\\n'\n",
      " '\\n'\n",
      " 'Palantir has offices in Zug, Switzerland; Mountain View, California; SFO, '\n",
      " 'San Francisco; Berlin, Germany; Shanghai, China; London, England; Tokyo, '\n",
      " 'Japan; Sydney, Australia; Dublin, Ireland; Budapest, Hungary; and Munich, '\n",
      " 'Germany. Its satellite office in Singapore supports operations in Indonesia, '\n",
      " 'the Philippines, and Malaysia.\\n'\n",
      " 'Related posts:<|endoftext|>')\n"
     ]
    }
   ],
   "source": [
    "p = \"\"\"Palantir has offices in which cities?\"\"\"\n",
    "o = model.generate(p, max_new_tokens=220)\n",
    "pprint(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f7df9b7d85d80cc4_*_of_00004.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done one\n",
      "Number of batches: 937\n",
      "done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loop \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 79.17 GiB total capacity; 77.68 GiB already allocated; 3.81 MiB free; 77.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb Cell 26\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tokens \u001b[39m=\u001b[39m test_string_trained_tokens\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m overfits \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,overfitMax):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39m#print(\"overfitting on \"+str(overfits))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   logits \u001b[39m=\u001b[39m model(tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   loss \u001b[39m=\u001b[39m lm_cross_entropy_loss(logits, tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_v0.0.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/HookedTransformer.py:325\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    322\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    323\u001b[0m         )\n\u001b[0;32m--> 325\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    326\u001b[0m         residual,\n\u001b[1;32m    327\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    328\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    331\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/components.py:765\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m add_head_dimension(shortformer_pos_embed)\n\u001b[1;32m    761\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_attn_out(\n\u001b[1;32m    762\u001b[0m     \u001b[39m# hook the residual stream states that are used to calculate the \u001b[39;00m\n\u001b[1;32m    763\u001b[0m     \u001b[39m# queries, keys and values, independently. \u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[39m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    766\u001b[0m         query_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(query_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    767\u001b[0m         key_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(key_input) \u001b[39m+\u001b[39;49m (\u001b[39m0.0\u001b[39;49m \u001b[39mif\u001b[39;49;00m shortformer_pos_embed \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m shortformer_pos_embed),\n\u001b[1;32m    768\u001b[0m         value_input \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(value_input),\n\u001b[1;32m    769\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache_entry,\n\u001b[1;32m    770\u001b[0m     )\n\u001b[1;32m    771\u001b[0m )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m    773\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[1;32m    774\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[1;32m    775\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebooks/transformer_lens/components.py:358\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     qkv_einops_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    357\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_q(\n\u001b[0;32m--> 358\u001b[0m     einsum(\n\u001b[1;32m    359\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mqkv_einops_string\u001b[39m}\u001b[39;49;00m\u001b[39m, head_index d_model d_head \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m        -> batch pos head_index d_head\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    361\u001b[0m         query_input,\n\u001b[1;32m    362\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_Q,\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_Q\n\u001b[1;32m    365\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_k(\n\u001b[1;32m    367\u001b[0m     einsum(\n\u001b[1;32m    368\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_K\n\u001b[1;32m    374\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    375\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_v(\n\u001b[1;32m    376\u001b[0m     einsum(\n\u001b[1;32m    377\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mqkv_einops_string\u001b[39m}\u001b[39;00m\u001b[39m, head_index d_model d_head \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_V\n\u001b[1;32m    383\u001b[0m )  \u001b[39m# [batch, pos, head_index, d_head]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[39m=\u001b[39m get_backend(operands[\u001b[39m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[39m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49meinsum(new_equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meinsum\u001b[39m(\u001b[39mself\u001b[39m, equation, \u001b[39m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49meinsum(equation, \u001b[39m*\u001b[39;49moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 79.17 GiB total capacity; 77.68 GiB already allocated; 3.81 MiB free; 77.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1\n",
    "log_every = 1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "overfitMax=1\n",
    "#model_cfg = Config(debug=False, d_model=256, n_heads=4, d_head=64, d_mlp=1024, n_layers=2, n_ctx=256, d_vocab=reference_gpt2.cfg.d_vocab)\n",
    "\n",
    "\n",
    "optimizer_copy = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "print(\"done one\")\n",
    "losses = []\n",
    "\n",
    "#print(dataset)\n",
    "#print(dataset[0]['text'][:100])\n",
    "\n",
    "tokens_dataset = tokenize_and_concatenate(dataset, model.tokenizer, streaming=False, max_length=cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(\"Number of batches:\", len(data_loader))\n",
    "#test_string_trained = \"Hello world this is a test of overfitting\"\n",
    "print(\"done\")\n",
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()\n",
    "#loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
    "#print(loss)\n",
    "#print(\"Loss as average prob\", (-loss).exp())\n",
    "#print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "#print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))\n",
    "#test_string_trained = \"Hello world this is a test of overfitting\"\n",
    "torch.set_grad_enabled(True)\n",
    "test_string_trained = \"\"\"\n",
    "German police and security services say they are preparing for Ukrainian President Volodymyr Zelensky to visit Berlin this month, a trip scheduled for May 13-14, marking Zelensky's first such visit to Germany since the war with Russia began.\n",
    "\n",
    "It's at the invitation of German Chancellor Olaf Scholz, with Scholz's office not yet having made the official announcement previewing the state visit. But the timing comes amid rising tensions between Kiev and Berlin, despite German Leopard II tanks now being transferred to Ukrainian forces.\n",
    "\"\"\"\n",
    "test_string_trained_tokens = model.to_tokens(test_string_trained, prepend_bos=True).cuda()\n",
    "for epoch in range(num_epochs):\n",
    "    #for c, batch in tqdm.tqdm(enumerate(data_loader)):\n",
    "    c=0\n",
    "    for c in range(0,overfitMax):\n",
    "        c += 1\n",
    "        if c > max_steps:\n",
    "              break\n",
    "        print(\"training loop \")\n",
    "        \n",
    "        #the_list = list(model.tokenizer.batch_decode(test_string_trained_tokens))\n",
    "        #print(the_list)\n",
    "        #tokens = batch['tokens'].cuda()\n",
    "        tokens = test_string_trained_tokens\n",
    "        for overfits in range(0,overfitMax):\n",
    "          #print(\"overfitting on \"+str(overfits))\n",
    "          logits = model(tokens)\n",
    "          loss = lm_cross_entropy_loss(logits, tokens)\n",
    "          loss.backward()\n",
    "          optimizer_copy.step()\n",
    "          optimizer_copy.zero_grad()\n",
    "          losses.append(loss.item())\n",
    "          print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "          # if c % log_every == 0:\n",
    "          #     print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "          #     #print(\"now testing model with:\\n    \" + test_string_test + \"\\n and expect:\\n    \"+test_string_expect)\n",
    "          #     test_string_test = \"abcdefghijkl\"\n",
    "          #     test_string_test_tokens = model.to_tokens(test_string_test, prepend_bos=True).cuda()\n",
    "          #     test_string_test_out = test_string_test\n",
    "          #     #pprint(list(enumerate(list(reference_gpt2_copy.tokenizer.batch_decode(reference_gpt2_copy.to_tokens(test_string_test_out)[0])))))\n",
    "          #     the_list_two = list()\n",
    "          #     pprint(list(enumerate(list(model.tokenizer.batch_decode(reference_gpt2_copy.to_tokens(test_string_test_out, prepend_bos=True)[0])))))\n",
    "          #     for i in range(0,1):\n",
    "                \n",
    "                \n",
    "          #       test_tokens = model.to_tokens(test_string_test_out, prepend_bos=True).cuda()\n",
    "                \n",
    "                \n",
    "          #       demo_logits = model(test_tokens)\n",
    "          #       #the_list = list(zip(reference_gpt2.to_str_tokens(test_string), reference_gpt2.tokenizer.batch_decode(demo_logits.argmax(dim=-1)[0])))\n",
    "          #       test_string_test_out += model.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "          #       the_list_two = list(model.tokenizer.batch_decode(demo_logits.argmax(dim=-1)[0]))\n",
    "          #       #print(str(i)+\"th loop\")\n",
    "          #       #print(list(enumerate(the_list)))\n",
    "          #       #test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "              \n",
    "          #     print(test_string_test)\n",
    "          #     print(\"AND THE OUTPUT\")\n",
    "          #     print(test_string_test_out)\n",
    "          #     #print(\"##\")\n",
    "          #     #print(\"\".join(the_list))\n",
    "          #     #print(list(enumerate(the_list_two)))\n",
    "          #     pprint(\"###########################################################\")\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
