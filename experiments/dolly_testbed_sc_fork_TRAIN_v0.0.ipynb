{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Working with Dolly\n",
    "## Last Updated $DATE -  $AUTHOR.\n",
    "\n",
    "```\n",
    "Summary of High Level Research Question\n",
    "```\n",
    "\n",
    "Try to scope your experiments such you can answer your research question in 1-3 hours.\n",
    "This is an ideal time block to enter flow / deep work, but short enough that you will still feel \n",
    "motivated by a relatively tight feedback loop.\n",
    "\n",
    "If a problem seems like it needs more time that that, \n",
    "\n",
    "### High Level Experiment Design\n",
    "\n",
    "## Goals:\n",
    "```\n",
    "List of specific goals that this experiment seeks to achieve.\n",
    "\n",
    "This should fall under a few categories:\n",
    "- Development of Intuition about a _specific_ topic\n",
    "- Novel Research or Insight that could lead to a publishable result\n",
    "- Meaningfully explore a topic which could lead to an improvement in product\n",
    "\n",
    "Guiding principles should understanding, insight, and value creation.\n",
    "```\n",
    "\n",
    "## Tasks & Experiment Design\n",
    "\n",
    "```\n",
    "A list of specific tasks that are going to be tested \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "```\n",
    "Document high level research findings and how\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
      "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-7ggb70o7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-7ggb70o7\n",
      "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 0ffcc8ad647d9e991f4c2596557a9d7475617773\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.6.1)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.28.1)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.15.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.5.0)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (0.2.15)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (2.12.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (13.2.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.9/dist-packages (from transformer-lens==0.0.0) (1.23.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.12.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.18.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (5.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.3)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.5.1)\n",
      "Requirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (2.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.9/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (66.1.1)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.30)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.19.6)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.14.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.10)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: circuitsvis in /usr/local/lib/python3.9/dist-packages (1.39.1)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (5.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.23.4)\n",
      "Requirement already satisfied: torch<2.0,>=1.10 in /usr/local/lib/python3.9/dist-packages (from circuitsvis) (1.12.1+cu116)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<2.0,>=1.10->circuitsvis) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (5.14.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from plotly) (23.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly) (8.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install things into ENV\n",
    "# TODO: Setup up a container and push to docker that contains all these\n",
    "%pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "%pip install circuitsvis\n",
    "%pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Set of Imports for MI Research\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch configuration for inference based experiments\n",
    "# NOTE: Mark as False if you want to do any kind of training \n",
    "#       as part of your experimentation\n",
    "\n",
    "INFERENCE_ONLY_EXPERIMENT = True\n",
    "if INFERENCE_ONLY_EXPERIMENT:\n",
    "    torch.set_grad_enabled(False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d2e13088-9d30\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d2e13088-9d30\",\n",
       "      Hello,\n",
       "      {\"name\": \"Vivek\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7ff77c44efd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Circuit Visualizations\n",
    "# TODO: Explore building out our own packages / tooling\n",
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Vivek\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded hf_model, hooking transformer into TransformerLens!\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50432,\n",
      " 'd_vocab_out': 50432,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n",
      "Loaded pretrained model into HookedTransformer!\n",
      "Model loss: tensor(4.5184, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Load & Run a Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"databricks/dolly-v2-7b\")\n",
    "\n",
    "print(\"Loaded hf_model, hooking transformer into TransformerLens!\")\n",
    "# model = HookedTransformer.from_pretrained(\n",
    "#     \"EleutherAI/pythia-6.9b-deduped\",\n",
    "#     center_unembed=False,\n",
    "#     center_writing_weights=False,\n",
    "#     fold_ln=False,\n",
    "#     refactor_factored_attn_matrices=True,\n",
    "#     hf_model=hf_model\n",
    "# )\n",
    "\n",
    "### Janky Shit\n",
    "### TODO: Figure out how this library actually works and make this a cleaner integration.\n",
    "import transformer_lens.loading_from_pretrained as loading\n",
    "# Get the model name used in HuggingFace, rather than the alias.\n",
    "official_model_name = loading.get_official_model_name(\"EleutherAI/pythia-6.9b-deduped\")\n",
    "\n",
    "\n",
    "# Load the config into an HookedTransformerConfig object. If loading from a\n",
    "# checkpoint, the config object will contain the information about the\n",
    "# checkpoint\n",
    "cfg = loading.get_pretrained_model_config(\n",
    "    official_model_name,\n",
    "    checkpoint_index=None,\n",
    "    checkpoint_value=None,\n",
    "    fold_ln=False,\n",
    "    device=device,\n",
    "    n_devices=1,\n",
    ")\n",
    "print(cfg)\n",
    "cfg.d_vocab = 50280\n",
    "cfg.d_vocab_out = 50280\n",
    "print(cfg)\n",
    "\n",
    "\n",
    "# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the HookedTransformer parameter names.\n",
    "state_dict = loading.get_pretrained_state_dict(\n",
    "    official_model_name, cfg, hf_model\n",
    ")\n",
    "\n",
    "# Create the HookedTransformer object\n",
    "model = HookedTransformer(cfg, tokenizer=tokenizer)\n",
    "\n",
    "model.load_and_process_state_dict(\n",
    "    state_dict,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    refactor_factored_attn_matrices=False,\n",
    "    move_state_dict_to_device=True,\n",
    ")\n",
    "\n",
    "print(f\"Loaded pretrained model into HookedTransformer!\")\n",
    "\n",
    "model_description_text = \"\"\"For this demo notebook we'll look at Dolly v2. It is based on pythia 6.9b, but we use the weights for dolly v2. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
    "# return_type of model can be loss, logits, both, or none!\n",
    "loss = model(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 128,\n",
      " 'd_mlp': 16384,\n",
      " 'd_model': 4096,\n",
      " 'd_vocab': 50280,\n",
      " 'd_vocab_out': 50280,\n",
      " 'device': 'cuda',\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.0125,\n",
      " 'model_name': 'pythia-6.9b-deduped',\n",
      " 'n_ctx': 2048,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 32,\n",
      " 'n_layers': 32,\n",
      " 'n_params': 6442450944,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPTNeoXForCausalLM',\n",
      " 'parallel_attn_mlp': True,\n",
      " 'positional_embedding_type': 'rotary',\n",
      " 'rotary_dim': 32,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'EleutherAI/pythia-6.9b-deduped',\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "# DOLLY V2 - 7B Config\n",
    "pprint(model.cfg)\n",
    "\n",
    "# Transformer Lens Note:\n",
    "# get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token\n",
    "# are all methods that are added to the model object by TransformerLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from easy_transformer.utils import get_corner, gelu_new, tokenize_and_concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Logit Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/NeelNanda___parquet/NeelNanda--pile-10k-72f566e9f7c464ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f7df9b7d85d80cc4_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "from pprint import pprint\n",
    "batch_size = 8\n",
    "num_epochs = 1\n",
    "max_steps = 1\n",
    "log_every = 1\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-2\n",
    "overfitMax=1\n",
    "\n",
    "#import gc\n",
    "\n",
    "#gc.collect()\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "tokens_dataset = tokenize_and_concatenate(dataset, model.tokenizer, streaming=False, max_length=cfg.n_ctx, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset, batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n",
    "optimizer_copy = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Step: 1, Loss: 4.1990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cfg.debug = False\n",
    "print(\"starting\")\n",
    "torch.set_grad_enabled(True)\n",
    "test_string_trained = \"\"\"German police and security services\"\"\"\n",
    "c=1\n",
    "#print(model.tokenizer.batch_decode(logits).shape)\n",
    "tokens = model.to_tokens(test_string_trained, prepend_bos=True).cuda()\n",
    "\n",
    "logits, loss = model(tokens, return_type=\"both\")\n",
    "#print(logits.shape)\n",
    "#print(tokens.shape)\n",
    "#lossF = torch.nn.CrossEntropyLoss()\n",
    "#loss = lossF(logits, tokens)\n",
    "\n",
    "#print(list(enumerate(list(model.tokenizer.batch_decode(logits)))))\n",
    "model.zero_grad(set_to_none=True)\n",
    "loss.backward()\n",
    "#optimizer_copy.step()\n",
    "#optimizer_copy.zero_grad()\n",
    "#losses.append(loss.item())\n",
    "#print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "#logits, loss = model(tokens, return_type=\"both\")\n",
    "#print(model.tokenizer.batch_decode(logits))\n",
    "#losses.append(loss.item())\n",
    "print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Tensor\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_TRAIN_v0.0.ipynb Cell 27\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_TRAIN_v0.0.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#print(model.tokens_to_residual_directions(tokens))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_TRAIN_v0.0.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#print(model.tokenizer.batch_decode(\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/stephencohenfresh/Downloads/mi_research/experiments/dolly_testbed_sc_fork_TRAIN_v0.0.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(model\u001b[39m.\u001b[39;49mto_tokens(logits)\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[0;32m/notebooks/transformer_lens/HookedTransformer.py:440\u001b[0m, in \u001b[0;36mHookedTransformer.to_tokens\u001b[0;34m(self, input, prepend_bos, move_to_device, truncate)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbos_token \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbos_token \u001b[39m+\u001b[39m string \u001b[39mfor\u001b[39;00m string \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m]\n\u001b[1;32m    441\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    442\u001b[0m     \u001b[39minput\u001b[39m, \n\u001b[1;32m    443\u001b[0m     return_tensors \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m     add_special_tokens \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mname_or_path\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mfacebook/opt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m  \u001b[39m# As we manually add the BOS token\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     )[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n",
      "File \u001b[0;32m/notebooks/transformer_lens/HookedTransformer.py:440\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbos_token \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    439\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mbos_token \u001b[39m+\u001b[39;49m string \u001b[39mfor\u001b[39;00m string \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m]\n\u001b[1;32m    441\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    442\u001b[0m     \u001b[39minput\u001b[39m, \n\u001b[1;32m    443\u001b[0m     return_tensors \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m     add_special_tokens \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mname_or_path\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mfacebook/opt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m  \u001b[39m# As we manually add the BOS token\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     )[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"Tensor\") to str"
     ]
    }
   ],
   "source": [
    "#print(model.tokens_to_residual_directions(tokens))\n",
    "#print(model.tokenizer.batch_decode(\n",
    "print(model.to_tokens(logits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
